{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from Some_functions.ipynb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jrnoo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from word_frequency.ipynb\n"
     ]
    }
   ],
   "source": [
    "#If any of these imports don't work, you probably need to do a pip install\n",
    "import import_ipynb #This is the module that lets you import ipynbs as if they are modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk \n",
    "from sklearn.datasets import load_files\n",
    "nltk.download('stopwords')\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import textstat as ts \n",
    "from Some_functions import phrases, phrase_count, dataframeizer, Ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCAphrases(df,n,numfeatures): # n is the no. of words per phrase, numfeatures is how many components you want\n",
    "    speechphrases = [phrases(text,n) for text in df[\"Transcript\"]] #This gives us a list of lists of phrases\n",
    "    allphrases = [] \n",
    "    for i in speechphrases: #adds all elements of i to allphrases,\n",
    "        allphrases += i     #ultimately creating one list of all phrases used in the speeches\n",
    "    phraselist = np.unique(allphrases) #gets rid of duplicates\n",
    "    newdf = df.copy()\n",
    "    for phrase in phraselist: #adds a column for each unique phrase giving the no of times that phrase is used in \n",
    "        newdf[phrase] = newdf[\"Transcript\"].apply(lambda text: phrases(text,n).count(phrase)) # each speech\n",
    "    labels = df[\"President\"]\n",
    "    features = newdf.drop([\"President\",\"Transcript\"],axis=1)\n",
    "    train_vectors,test_vectors,train_labels,test_labels=train_test_split(features,labels)\n",
    "    pca = PCA(n_components=numfeatures, whiten=True)\n",
    "    _ = pca.fit(train_vectors)\n",
    "    newfeatures = pca.transform(features)\n",
    "    return pd.DataFrame(newfeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pseudoPCA(df,n,numfeatures): \n",
    "    speechphrases = [phrases(text,n) for text in df[\"Transcript\"]]\n",
    "    allphrases = []\n",
    "    for i in speechphrases:\n",
    "        allphrases += i\n",
    "    phraselist = np.unique(allphrases)\n",
    "    newdf = df.copy()\n",
    "    labels = df[\"President\"]\n",
    "    presidents = np.unique(labels)\n",
    "    stds = []\n",
    "    phrasestds = {}\n",
    "    for phrase in phraselist:\n",
    "        newdf[phrase] = newdf[\"Transcript\"].apply(lambda text: phrases(text,n).count(phrase))\n",
    "        aves = []\n",
    "        for pres in presidents:\n",
    "            rows = newdf[newdf[\"President\"]==pres] # pick out only the rows corresponding to each president\n",
    "            aves.append(sum(rows[phrase])/len(rows[phrase])) #avereage number of times each pres uses the phrase\n",
    "        mean = sum(aves)/len(aves)\n",
    "        devs = np.array(aves)-mean\n",
    "        var = sum(devs**2)/len(devs)\n",
    "        std = var**0.5 # We calculate the standard deviation of the presidents' average no of phrase uses\n",
    "        stds.append(std)\n",
    "        phrasestds[phrase] = std\n",
    "    stds.sort(reverse=True)\n",
    "    cutoff = stds[n-1] #Only the phrases in the top n std devs will be picked as features \n",
    "    for phrase in phraselist:\n",
    "        if phrasestds[phrase] < cutoff:\n",
    "            newdf.drop(phrase,axis=1) # get rid of features with std dev too small - they are less significant\n",
    "    features = newdf.drop([\"President\",\"Transcript\"],axis=1)\n",
    "    return features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
